#!/bin/bash

# The `environment` hook will run before all other commands, and can be used
# to set up secrets, data, etc. Anything exported in hooks will be available
# to the build script.
#
# For example:
#
# export SECRET_VAR=token

set -euo pipefail # exit on failure or unset variable

export SLURM_BUILDKITE_PATH="$(dirname "${BUILDKITE_BIN_PATH}")"
export GIT_SSH_COMMAND="ssh -i ${SLURM_BUILDKITE_PATH}/.ssh/id_rsa -o IdentitiesOnly=yes"
export CI_BUILD_DIR="${BUILDKITE_BUILD_PATH}/${BUILDKITE_PIPELINE_SLUG}/${BUILDKITE_BUILD_NUMBER}" 
export BUILDKITE_BUILD_CHECKOUT_PATH="${CI_BUILD_DIR}/${BUILDKITE_PIPELINE_SLUG}"
export BUILDKITE_API_TOKEN=$(cat "${SLURM_BUILDKITE_PATH}/.buildkite_token")

# Slurm Job Info
echo "--- Slurm Job ID: ${SLURM_JOB_ID}"
scontrol show job ${SLURM_JOB_ID}

module purge
module load git/2.26.0

# this is set to avoid rare race conditions on the same node with concurrent (step) jobs
export OMPI_MCA_orte_tmpdir_base="/tmp/slurm-buildkite/${BUILDKITE_STEP_KEY}"


case "${BUILDKITE_AGENT_META_DATA_CONFIG}" in
    # standard configurations
   ""|default)
        module load ${BUILDKITE_AGENT_META_DATA_MODULES:-}
        export JULIA_DEPOT_PATH="${JULIA_DEPOT_PATH:-$CI_BUILD_DIR/depot/default}"
        ;;

    init)
        echo "--- init env configuration"
        ;;

    cpu)
        echo "--- cpu env configuration"
        module load singularity/3.5.2
        module load python3/${PYTHON_VERSION:=3.8.5}
        module load julia/${JULIA_VERSION:=1.7.1}
        if [ "${MPI_IMPL:=mpich}" == "mpich" ]; then
            module load mpich/${MPICH_VERSION:=4.0.0} hdf5/1.12.1 netcdf-c/4.8.1
        else
            module load openmpi/${OPENMPI_VERSION:=4.1.1}
            if [ "${OPENMPI_VERSION:=4.1.1}" == "4.1.1" ]; then
                module load hdf5/1.12.1-ompi411
            else
                module load hdf5/1.12.1
            fi
        fi

        export JULIA_DEPOT_PATH="${JULIA_DEPOT_PATH:=$CI_BUILD_DIR/depot/cpu}"
        export JULIA_CUDA_USE_BINARYBUILDER=false
        export JULIA_MPI_BINARY=system
        export JULIA_HDF5_PATH=""

        # this is set to not use shared memory segments as a btl transport
        # they do not get cleaned up on the cluster
        # export OMPI_MCA_btl="self,tcp"
        export CLIMATEMACHINE_SETTINGS_OUTPUT_DIR="${CI_BUILD_DIR}/output/${BUILDKITE_STEP_KEY}"

        echo "--- slurm job configuration"
        sacct -a -X -j ${SLURM_JOB_ID} --format=JobID,AllocCPUs,nnodes,nodelist
        ;;

    cpu-test)
        echo "--- cpu test env configuration"
        module load singularity/3.5.2
        module load python3/${PYTHON_VERSION:=3.8.5}
        module load julia/${JULIA_VERSION:=1.7.1}
        if [ "${MPI_IMPL:=mpich}" == "mpich" ]; then
            module load mpich/${MPICH_VERSION:=4.0.0} hdf5/1.12.1 netcdf-c/4.8.1
        else
            module load openmpi/${OPENMPI_VERSION:=4.1.1} hdf5/1.12.1-ompi411
        fi

        export JULIA_DEPOT_PATH:="${CI_BUILD_DIR}/depot/cpu"
        export JULIA_CUDA_USE_BINARYBUILDER=false
        export JULIA_MPI_BINARY=system
        export JULIA_HDF5_PATH=""

        # export OMPI_MCA_btl="self,tcp"
        export OMPI_MCA_orte_tmpdir_base="/tmp/slurm-buildkite/${BUILDKITE_STEP_KEY}"
        export CLIMATEMACHINE_SETTINGS_OUTPUT_DIR="${CI_BUILD_DIR}/output/${BUILDKITE_STEP_KEY}"

        echo "--- slurm job configuration"
        sacct -a -X -j ${SLURM_JOB_ID} --format=JobID,AllocCPUs,nnodes,nodelist
        ;;

    gpu)
        echo "--- gpu env configuration"
        module load singularity/3.5.2
        module load python3/${PYTHON_VERSION:=3.8.5}
        module load julia/${JULIA_VERSION:=1.7.1}
        module load cuda/${CUDA_VERSION:=10.2} openmpi/${OPENMPI_VERSION:=4.0.4}_cuda-${CUDA_VERSION:=10.2} hdf5/1.10.1 netcdf-c/4.6.1

        export JULIA_DEPOT_PATH="${JUIA_DEPOT_PATH:=$CI_BUILD_DIR/depot/gpu}"
        export JULIA_CUDA_USE_BINARYBUILDER=false
        export JULIA_MPI_BINARY=system

        # export OMPI_MCA_btl="self,tcp"
        export OMPI_MCA_orte_tmpdir_base="/tmp/slurm-buildkite/${BUILDKITE_STEP_KEY}"
        export CLIMATEMACHINE_SETTINGS_OUTPUT_DIR="${CI_BUILD_DIR}/output/${BUILDKITE_STEP_KEY}"

        echo "--- slurm job configuration"
        sacct -a -X -j ${SLURM_JOB_ID} --format=JobID,AllocCPUs,nnodes,nodelist

        echo "--- gpu device configuration"
        nvidia-smi -q
        ;;

    gpu-test)
        echo "--- gpu test env configuration"
        module load singularity/3.5.2
        module load python3/${PYTHON_VERSION:=3.8.5}
        module load julia/${JULIA_VERSION:=1.7.1}
        module load cuda/${CUDA_VERSION:=10.2} openmpi/${OPENMPI_VERSION:=4.0.4}_cuda-${CUDA_VERSION:=10.2} hdf5/1.10.1 netcdf-c/4.6.1

        export JULIA_DEPOT_PATH="${CI_BUILD_DIR}/depot/gpu"
        export JULIA_CUDA_USE_BINARYBUILDER=false
        export JULIA_MPI_BINARY=system

        # export OMPI_MCA_btl="self,tcp"
        export OMPI_MCA_orte_tmpdir_base="/tmp/slurm-buildkite/${BUILDKITE_STEP_KEY}"

        echo "--- slurm job configuration"
        sacct -a -X -j ${SLURM_JOB_ID} --format=JobID,AllocCPUs,nnodes,nodelist

        echo "--- gpu test device configuration"
        nvidia-smi -q
        ;;

    *)
        echo "agent config must init, cpu or gpu"
        ;;
esac

printf -- '--- ' && module list

if [ -n "${SLURM_GPUS_ON_NODE:-}" ]; then
    echo "--- GPUs available on ${HOSTNAME}"
    nvidia-smi -L
fi

# Add pipeline specific ENV variables
if [ "$BUILDKITE_PIPELINE_NAME" == "ClimateMachine-Docs" ]; then
    export DOCUMENTER_KEY=$(cat "$BUILDKITE_PATH/.climatemachine_documenter_key")
fi

export SLACK_TOKEN=$(cat "$BUILDKITE_PATH/.slack_token")
