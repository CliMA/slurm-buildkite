#!/bin/bash

# The `environment` hook will run before all other commands, and can be used
# to set up secrets, data, etc. Anything exported in hooks will be available
# to the build script.
#
# For example:
#
# export SECRET_VAR=token

set -euo pipefail # exit on failure or unset variable
#set -x  # show all the commands that are being executed

source /etc/profile.d/modules.sh

export SLURM_BUILDKITE_PATH="$(dirname "${BUILDKITE_BIN_PATH}")"
export SHARED_DEPOT="${BUILDKITE_BUILD_PATH}/shared_depot"
export GIT_SSH_COMMAND="ssh -i ${SLURM_BUILDKITE_PATH}/.ssh/id_rsa -o IdentitiesOnly=yes -o UserKnownHostsFile=${SLURM_BUILDKITE_PATH}/.ssh/known_hosts"
export CI_BUILD_DIR="${BUILDKITE_BUILD_PATH}/${BUILDKITE_PIPELINE_SLUG}/${BUILDKITE_BUILD_NUMBER}"
export BUILDKITE_BUILD_CHECKOUT_PATH="${CI_BUILD_DIR}/${BUILDKITE_PIPELINE_SLUG}"
export BUILDKITE_API_TOKEN=$(cat "${SLURM_BUILDKITE_PATH}/.buildkite_token")

export TMPDIR="/tmp/slurm-${SLURM_JOB_ID}"
srun --ntasks-per-node=1 --ntasks=${SLURM_JOB_NUM_NODES} mkdir -p "${TMPDIR}"
printenv TMPDIR

ulimit -c unlimited

# Slurm Job Info
echo "--- Slurm Job ID: ${SLURM_JOB_ID}"
scontrol show job ${SLURM_JOB_ID} --details
scontrol show node ${SLURM_JOB_NODELIST}

if $(grep -q "Red Hat" /etc/redhat-release);  then
    # We are on rhel9 nodes, we also want our modules
    export MODULEPATH="/groups/esm/modules:$MODULEPATH"

    module load git/2.39.3-gcc-11.3.1-zfr3sti
else
    # We are on the CentOS nodes
    module load git/2.26.0
fi


# this is set to avoid rare race conditions on the same node with concurrent (step) jobs
export OMPI_MCA_orte_tmpdir_base="$TMPDIR"

case "${BUILDKITE_AGENT_META_DATA_CONFIG}" in
    # standard configurations
   ""|default)
        module load ${BUILDKITE_AGENT_META_DATA_MODULES:-}
        export JULIA_DEPOT_PATH="${JULIA_DEPOT_PATH:-$CI_BUILD_DIR/depot/default}:${SHARED_DEPOT}"
        ;;

    init)
        echo "--- init env configuration"
        ;;

    cpu)
        echo "--- cpu env configuration"
        module load singularity/3.5.2
        module load python3/${PYTHON_VERSION:=3.8.5}
        module load julia/${JULIA_VERSION:=1.10.0}
        if [ "${MPI_IMPL:=mpich}" == "mpich" ]; then
            module load mpich/${MPICH_VERSION:=4.0.0} hdf5/1.12.1 netcdf-c/4.8.1
        else
            module load openmpi/${OPENMPI_VERSION:=4.1.1}
            if [ "${OPENMPI_VERSION:=4.1.1}" == "4.1.1" ]; then
                module load hdf5/1.12.1-ompi411
            else
                module load hdf5/1.12.1
            fi
        fi

        export JULIA_DEPOT_PATH="${JULIA_DEPOT_PATH:=$CI_BUILD_DIR/depot/cpu}"
        export JULIA_CUDA_USE_BINARYBUILDER=false
        export JULIA_MPI_BINARY=system
        export JULIA_HDF5_PATH=""

        # this is set to not use shared memory segments as a btl transport
        # they do not get cleaned up on the cluster
        # export OMPI_MCA_btl="self,tcp"
        export CLIMATEMACHINE_SETTINGS_OUTPUT_DIR="${CI_BUILD_DIR}/output/${BUILDKITE_STEP_KEY}"

        echo "--- slurm job configuration"
        sacct -a -X -j ${SLURM_JOB_ID} --format=JobID,AllocCPUs,nnodes,nodelist
        ;;

    cpu-test)
        echo "--- cpu test env configuration"
        module load singularity/3.5.2
        module load python3/${PYTHON_VERSION:=3.8.5}
        module load julia/${JULIA_VERSION:=1.10.0}
        if [ "${MPI_IMPL:=mpich}" == "mpich" ]; then
            module load mpich/${MPICH_VERSION:=4.0.0} hdf5/1.12.1 netcdf-c/4.8.1
        else
            module load openmpi/${OPENMPI_VERSION:=4.1.1} hdf5/1.12.1-ompi411
        fi

        export JULIA_DEPOT_PATH:="${CI_BUILD_DIR}/depot/cpu"
        export JULIA_CUDA_USE_BINARYBUILDER=false
        export JULIA_MPI_BINARY=system
        export JULIA_HDF5_PATH=""

        # export OMPI_MCA_btl="self,tcp"
        export OMPI_MCA_orte_tmpdir_base="/tmp/slurm-buildkite/${BUILDKITE_STEP_KEY}"
        export CLIMATEMACHINE_SETTINGS_OUTPUT_DIR="${CI_BUILD_DIR}/output/${BUILDKITE_STEP_KEY}"

        echo "--- slurm job configuration"
        sacct -a -X -j ${SLURM_JOB_ID} --format=JobID,AllocCPUs,nnodes,nodelist
        ;;

    gpu)
        echo "--- gpu env configuration"
        module load singularity/3.5.2
        module load python3/${PYTHON_VERSION:=3.8.5}
        module load julia/${JULIA_VERSION:=1.10.0}
        module load cuda/${CUDA_VERSION:=12.2} openmpi/${OPENMPI_VERSION:=4.0.4}_cuda-${CUDA_VERSION:=12.2} hdf5/1.10.1 netcdf-c/4.6.1

        export JULIA_DEPOT_PATH="${JUIA_DEPOT_PATH:=$CI_BUILD_DIR/depot/gpu}"
        export JULIA_CUDA_USE_BINARYBUILDER=false
        export JULIA_MPI_BINARY=system

        # export OMPI_MCA_btl="self,tcp"
        export OMPI_MCA_orte_tmpdir_base="/tmp/slurm-buildkite/${BUILDKITE_STEP_KEY}"
        export CLIMATEMACHINE_SETTINGS_OUTPUT_DIR="${CI_BUILD_DIR}/output/${BUILDKITE_STEP_KEY}"

        echo "--- slurm job configuration"
        sacct -a -X -j ${SLURM_JOB_ID} --format=JobID,AllocCPUs,nnodes,nodelist

        echo "--- gpu device configuration"
        nvidia-smi -q
        ;;

    gpu-test)
        echo "--- gpu test env configuration"
        module load singularity/3.5.2
        module load python3/${PYTHON_VERSION:=3.8.5}
        module load julia/${JULIA_VERSION:=1.10.0}
        module load cuda/${CUDA_VERSION:=12.2} openmpi/${OPENMPI_VERSION:=4.0.4}_cuda-${CUDA_VERSION:=12.2} hdf5/1.10.1 netcdf-c/4.6.1

        export JULIA_DEPOT_PATH="${CI_BUILD_DIR}/depot/gpu"
        export JULIA_CUDA_USE_BINARYBUILDER=false
        export JULIA_MPI_BINARY=system

        # export OMPI_MCA_btl="self,tcp"
        export OMPI_MCA_orte_tmpdir_base="/tmp/slurm-buildkite/${BUILDKITE_STEP_KEY}"

        echo "--- slurm job configuration"
        sacct -a -X -j ${SLURM_JOB_ID} --format=JobID,AllocCPUs,nnodes,nodelist

        echo "--- gpu test device configuration"
        nvidia-smi -q
        ;;

    *)
        echo "agent config must init, cpu or gpu"
        ;;
esac

printf -- '--- ' && module list

if [ -n "${SLURM_GPUS_ON_NODE:-}" ]; then
    echo "--- GPUs available on ${HOSTNAME}"
    nvidia-smi -L
fi

# Add pipeline specific ENV variables
if [ "$BUILDKITE_PIPELINE_NAME" == "ClimateMachine-Docs" ]; then
    export DOCUMENTER_KEY=$(cat "$BUILDKITE_PATH/.climatemachine_documenter_key")
fi

export SLACK_TOKEN=$(cat "$BUILDKITE_PATH/.slack_token")
